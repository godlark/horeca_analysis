import gensim
import pandas as pd
import spacy
import stanza
from sklearn.cluster import AgglomerativeClustering
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np


# ðŸ“Œ 1. Wczytanie danych z pliku CSV
# Load data from CSV file
df = pd.read_csv("horeca_pomorskie_with_powiat.csv")
documents = df['name']

# ðŸ“Œ 2. Tokenizacja i lematyzacja tekstu w jÄ™zyku polskim
# Tokenize and lemmatize Polish text
nlp = stanza.Pipeline('pl', processors='tokenize,lemma')
spacy_nlp = spacy.load("pl_core_news_lg")
def stanza_lemmatize(text):
    doc = nlp(text)
    lemmas = []
    for sentence in doc.sentences:
        for word in sentence.words:
            if word.lemma.isalpha() and word.lemma.lower() not in spacy_nlp.Defaults.stop_words:
                lemmas.append(word.lemma.lower())
    return lemmas

tokenized_docs = [stanza_lemmatize(doc) for doc in documents]

# ðŸ“Œ 3. Wczytanie wczeÅ›niej wytrenowanych wektorÃ³w fastText (Polish)
# Load pre-trained fastText vectors (Polish)
model = gensim.models.fasttext.load_facebook_model("cc.pl.300.bin")


# ðŸ“Œ 4. Tworzenie listy unikalnych sÅ‚Ã³w zawartych w dokumentach (tylko te, ktÃ³re sÄ… w modelu)
# Create list of unique words in documents (only if in model)
unique_words = list(set(
    word for doc in tokenized_docs for word in doc if word in model.wv.key_to_index
))
print(unique_words)

# ðŸ“Œ 5. Dodanie listy zakazanych sÅ‚Ã³w (forbidden words) i lematyzacja ich
# Add list of forbidden words and lemmatize them
forbidden_words = ["krajowego", "planu", "odbudowy", "gospodarka", "dywersyfikacja",
                 "projekt", "wojewÃ³dztwo", "GdaÅ„sk", "pomorze", "innowacyjnoÅ›ci", "uodpornienie", "wzmocnienie",
                 "rozszerzenia", "zdywersyfikowanie", "wprowadzenie", "spÃ³Å‚ka", "odpowiedzialnoÅ›ciÄ…", "ograniczonÄ…"]
forbidden_words = [nlp(word).sentences[0].words[0].lemma.lower() for word in forbidden_words]
print(forbidden_words)
unique_words += forbidden_words

# ðŸ“Œ 6. Konwersja sÅ‚Ã³w do wektorÃ³w osadzeÅ„ (embeddings)
# Convert words to embeddings
embeddings = np.array([model.wv[word] for word in unique_words])

# ðŸ“Œ 7. Grupowanie sÅ‚Ã³w w klastry metodÄ… aglomeracyjnÄ… (cosine distance)
# Cluster words using AgglomerativeClustering (cosine distance)
clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.15, metric='cosine', linkage='average')
clusters = clustering.fit_predict(embeddings)

# ðŸ“Œ 8. Mapowanie sÅ‚Ã³w na ID klastrÃ³w
# Map words to cluster IDs
word_to_cluster = {word: cluster_id for word, cluster_id in zip(unique_words, clusters)}
forbidden_clusters = [word_to_cluster[forbidden_word] for forbidden_word in forbidden_words]
print(forbidden_clusters)

# ðŸ“Œ 9. Zamiana sÅ‚Ã³w w dokumentach na ID klastrÃ³w
# Replace words in documents with cluster IDs
cluster_docs = []
for doc in tokenized_docs:
    cluster_docs.append([str(word_to_cluster[word]) for word in doc if word in word_to_cluster])

# ðŸ“Œ 10. Obliczanie TF-IDF dla ID klastrÃ³w (jako tokenÃ³w)
# Run TF-IDF on cluster IDs (as tokens)
tfidf = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)
tfidf_matrix = tfidf.fit_transform(cluster_docs)

# ðŸ“Œ 11. Tworzenie odwrotnej mapy: ID klastra â†’ przykÅ‚adowe sÅ‚owo
# Inverse mapping: cluster_id â†’ example word
cluster_to_word = {}
for word, cluster_id in word_to_cluster.items():
    if cluster_id not in cluster_to_word:
        cluster_to_word[cluster_id] = word

feature_names = tfidf.get_feature_names_out()  # cluster IDs as strings

# ðŸ“Œ 12. Wyznaczenie 5 najwaÅ¼niejszych sÅ‚Ã³w (klastrÃ³w) dla kaÅ¼dego dokumentu
# Get top 5 important clusters (words) for each document
top1_list, top2_list, top3_list, top4_list, top5_list = [], [], [], [], []
for doc_idx, vector in enumerate(tfidf_matrix):
    print(f"\nDocument {doc_idx + 1}:")
    scores = vector.toarray().flatten()

    # Sortowanie malejÄ…co po wyniku tf-idf
    # Sort indices by descending tf-idf score
    sorted_indices = scores.argsort()[::-1]

    count = 0
    doc_top_words = ["", "", "", "", ""]  # placeholders for top1..top5
    for idx in sorted_indices:
        cluster_id_str = feature_names[idx]
        cluster_id = int(cluster_id_str)

        if cluster_id in forbidden_clusters:
            continue  # skip forbidden clusters

        example_word = cluster_to_word.get(cluster_id, "N/A")
        doc_top_words[count] = example_word
        score = scores[idx]
        count += 1
        print(f"  Top {count} cluster: ID={cluster_id}, example word='{example_word}', tf-idf={score:.4f}")

        if count >= 5:  # print top 5 allowed clusters
            break

    top1_list.append(doc_top_words[0])
    top2_list.append(doc_top_words[1])
    top3_list.append(doc_top_words[2])
    top4_list.append(doc_top_words[3])
    top5_list.append(doc_top_words[4])

# ðŸ“Œ 13. Dodanie kolumn z wynikami do DataFrame
# Add results to DataFrame
df["tfidf1"] = top1_list
df["tfidf2"] = top2_list
df["tfidf3"] = top3_list
df["tfidf4"] = top4_list
df["tfidf5"] = top5_list

# ðŸ“Œ 14. Zapis danych do CSV
# Save results to CSV
df.to_csv("horeca_pomorskie_with_powiat_with_tfidf.csv", index=False)